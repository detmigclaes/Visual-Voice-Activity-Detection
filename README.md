# Visual-Voice-Activity-Detection V-VAD
Project for Computer Vision F23

This project will explore methods for Voice Activity Detection.
This will include a review of methods for V-VAD and a review of the different datasets that are used to train and validate the models. For this, the following papers will be discussed:

* Visual Voice Activity Detection in the Wild by Foteini Patrona, Alexandros Iosifidis, Anastasios Tefas, Nikolaos Nikolaidis, and Ioannis Pitas.
    
* Learning Visual Voice Activity Detection with an Automatically Annotated Dataset by Sylvain Guy, Stephane Lathuiliére, Pablo Mesejoz, and Radu Horaud. 
    
* Audio-video fusion strategies for active speaker detection in meetings by Lionel Pibre, Francisco Madrigal, Cyrille Equoy, Fredéric Leraslé, Thomas Pellegrini, Julien Pinquier, Isabelle Ferrané.
    
* Vision-based Active Speaker Detection in Multiparty Interactions by Kalin Stefanov, Jonas Beskow, Giampiero Salvi.

* S-VVAD: Visual Voice Activity Detection by Motion Segmentation by Muhammad Shahid, Cigdem Beyan, Vittorio Murino.
    
* RealVAD: A Real-World Dataset and A Method for Voice Activity Detection by Body Motion Analysis by Cigdem Beyan, Muhammad Shahid, Vittorio Murino.

We will also verify the code used by S-VVAD and RealVAD by running the models on the RealVAD dataset. 
This dataset has its origin in this YouTube video of a panel debate: www.youtube.com/watch?v=51pRTOIso4U 

We will also aim to verify the performance of the models on the Colombia dataset, whos nature is similar to that of RealVAD. 
This dataset can be found here: https://www.youtube.com/watch?v=6GzxbrO0DHM


By Oliver Fridorf and Claes Eske Harbo Jensen
